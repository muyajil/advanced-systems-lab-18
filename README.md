# ASL Fall 2018 Mohammed Ajil README

## Folder structure

## experimentation

### experimentation/tests

In here we can find a folder for each experiment.
The folder names correspond to the test names used later.
In each folder we can find a config.json which reflects the configuration of the test, i.e. how many client machines, servers etc.

Also in each folder we can find the data for the different runs. 
Each experiment run has a run_id which will also be referenced later.
For each run we have again two folders inside.

#### experimentation/tests/[TEST_NAME]/[RUN_ID]/logs

In these folders we can find the logs for each executed memtier workload.
The folder structure encodes the different runs.
We two different types of logs: The main logs from the middleware and memtier, and the client stats files generated by memtier.

In the logs we reference client servers with ids 4, 5, and 6 these are not additional machines, but again the the id of the memtier instance. When we run two clients per machine we will find the the second memtier instance on the first server under id 4, on the second server id 5 and finally on the third id 6.

The main logs can be fonud under this path:

`[NUM_WORKERS]/[NUM_CLIENTS]/[WORKLOAD]/[REPETITION]/[middleware|memtier]/[Id].log`

The client stats files can be found under this path:

`[NUM_WORKERS]/[NUM_CLIENTS]/[WORKLOAD]/[REPETITION]/[middleware|memtier]/[Id]_[memtier instance]-x-x-x.csv`

#### experimentation/asltesting

This is a python module that contains all the code needed for installing the servers, executing the experiments and doing the analysis.

- Install environment: 
  - `cd experimenation && pipenv --three install` or `cd experimentation && pip install -r requirements.txt`

- Install relevant software on servers:
  - Modify `servers.json` to reflect your configuration 
  - `python -m asltesting.run --install`

- Execute experiment(s):
  - `python -m asltesting.run --run_id [RUN_ID] --test_names [TEST_NAME_1] ([TEST_NAME_2] ...)`

- Generate plots for experiment(S):
  - `python -m asltesting.run --run_id [RUN_ID] --test_names [TEST_NAME_1] ([TEST_NAME_2] ...) --plot`

- Generate submission plots and sampled logs:
  - `python -m asltesting.run --run_id [RUN_ID] --test_names [TEST_NAME_1] ([TEST_NAME_2] ...) --prepare_submission`

- Generate 2K analysis tables:
  - `python -m asltesting.run --run_id [RUN_ID] --2k_analysis`
  - The tables will be printed to stdout.

- Run an experiment locally:
  - Requires Docker and Docker Compose to be installed
  - Requires further a built Docker image of `memcached_benchmark`
  - `python -m asltesting.run --run_id [RUN_ID] --test_names [TEST_NAME_1] ([TEST_NAME_2] ...) --local`

## illustrations

This folder contains all the illustrations for the report.

Also in this folder we can find Jupyter Notebooks for the Queueing Models, since we used the aggregated data that were used for the plots these are also here.

### illustrations/plots

This folder contains all the plots referenced in the report.
Also in here we can find the transformed data that directly was used to generate the plots.

## submission_logs

In here we can find the logs for the submission. 
After preparing the submission the memtier logs for the specific submission will be copied here.

However since the middleware logs are a too large we will just sample them and save them in this folder.

The folder structure follows the same pattern as before and should be rather self explanatory.